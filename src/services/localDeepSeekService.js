/**
 * Local DeepSeek Model Service
 * Integrates with locally hosted DeepSeek models
 */

const { spawn } = require('child_process');
const path = require('path');
const fs = require('fs').promises;

class LocalDeepSeekService {
    constructor() {
        this.venvPath = path.join(__dirname, '../../venv');
        this.pythonPath = path.join(this.venvPath, 'bin', 'python');
        this.modelsPath = path.join(__dirname, '../ai/models');
        this.isInitialized = false;
        this.models = {
            coder: null,
            reasoner: null
        };
    }

    /**
     * Initialize the local DeepSeek service
     */
    async initialize() {
        try {
            // Check if virtual environment exists
            await this.ensureVirtualEnvironment();
            
            // Check if models are available
            await this.checkModels();
            
            this.isInitialized = true;
            console.log('✅ Local DeepSeek service initialized successfully');
            return true;
        } catch (error) {
            console.error('❌ Failed to initialize Local DeepSeek service:', error.message);
            return false;
        }
    }

    /**
     * Ensure virtual environment is set up
     */
    async ensureVirtualEnvironment() {
        try {
            // Check if venv exists
            await fs.access(this.pythonPath);
            console.log('✅ Virtual environment found');
        } catch (error) {
            throw new Error('Virtual environment not found. Please set up Python virtual environment.');
        }
    }

    /**
     * Check if models are available
     */
    async checkModels() {
        try {
            // Create models directory if it doesn't exist
            await fs.mkdir(this.modelsPath, { recursive: true });
            
            // For now, we'll use a simulated local setup
            // In production, you would check for actual model files
            console.log('✅ Models directory ready');
        } catch (error) {
            throw new Error(`Failed to set up models directory: ${error.message}`);
        }
    }

    /**
     * Generate response using local DeepSeek models
     * @param {string} prompt - User input
     * @param {string} type - 'code' or 'reasoning'
     */
    async generateResponse(prompt, type = 'reasoning') {
        if (!this.isInitialized) {
            throw new Error('Service not initialized');
        }

        // Check if actual models are available
        const modelsAvailable = await this.checkLocalModels();
        
        if (modelsAvailable) {
            return this.generateWithLocalModels(prompt, type);
        } else {
            // Fallback to intelligent simulation
            return this.simulateLocalModel(prompt, type);
        }
    }

    /**
     * Simulate local model responses
     * This provides a realistic interface while you download actual models
     */
    async simulateLocalModel(prompt, type) {
        const lowerPrompt = prompt.toLowerCase();
        
        if (type === 'code' || lowerPrompt.includes('code') || lowerPrompt.includes('function')) {
            return this.generateCodeResponse(prompt);
        } else {
            return this.generateReasoningResponse(prompt);
        }
    }

    /**
     * Generate code-focused responses
     */
    generateCodeResponse(prompt) {
        const codeExamples = {
            react: `import React, { useState } from 'react';

const ExampleComponent = () => {
  const [data, setData] = useState(null);
  
  return (
    <div className="example-component">
      <h2>Generated by Helios</h2>
      <p>Your component is ready!</p>
    </div>
  );
};

export default ExampleComponent;`,
            
            javascript: `// Generated by Helios - Sunny's AI Assistant
function processPayment(amount, currency) {
  try {
    const payment = {
      amount: parseFloat(amount),
      currency: currency.toUpperCase(),
      timestamp: new Date().toISOString(),
      status: 'pending'
    };
    
    console.log('Processing payment:', payment);
    return payment;
  } catch (error) {
    console.error('Payment processing error:', error);
    throw error;
  }
}`,
            
            python: `# Generated by Helios - Sunny's AI Assistant
def process_payment(amount, currency):
    """
    Process payment using Sunny's payment system
    """
    try:
        payment = {
            'amount': float(amount),
            'currency': currency.upper(),
            'timestamp': datetime.now().isoformat(),
            'status': 'pending'
        }
        
        print(f"Processing payment: {payment}")
        return payment
    except Exception as error:
        print(f"Payment processing error: {error}")
        raise`
        };
        
        const lowerPrompt = prompt.toLowerCase();
        let selectedCode = codeExamples.javascript;
        
        if (lowerPrompt.includes('react') || lowerPrompt.includes('component')) {
            selectedCode = codeExamples.react;
        } else if (lowerPrompt.includes('python')) {
            selectedCode = codeExamples.python;
        }
        
        return `I can help you with code generation! Here's what I've created based on your request:

\`\`\`${lowerPrompt.includes('python') ? 'python' : 'javascript'}
${selectedCode}
\`\`\`

This code is generated locally using Sunny's DeepSeek-Coder model. Would you like me to modify or extend this code in any way?`;
    }

    /**
     * Generate reasoning-focused responses
     */
    generateReasoningResponse(prompt) {
        const lowerPrompt = prompt.toLowerCase();
        
        // Helios identity and company knowledge
        if (lowerPrompt.includes('who are you') || lowerPrompt.includes('what are you')) {
            return `I'm Helios, Sunny's advanced AI assistant. I run on our proprietary models developed in-house at Sunny, which means your data stays completely private and secure on your infrastructure. I was built by Samuel Mbugua, Sunny's founder and ML engineer, to help with code generation, payment processing insights, and technical assistance. I use Sunny's locally-hosted Helios Code and Helios Reason models for intelligent assistance.`;
        }
        
        if (lowerPrompt.includes('founder') || lowerPrompt.includes('samuel')) {
            return `Samuel Mbugua is the founder of Sunny and a skilled ML engineer. He created Sunny to revolutionize payment processing through AI-powered technology, and he developed me (Helios) along with our proprietary AI models to assist developers and businesses with their technical needs. Samuel's vision is to democratize financial services worldwide using cutting-edge machine learning models developed here at Sunny.`;
        }
        
        if (lowerPrompt.includes('sunny') && (lowerPrompt.includes('company') || lowerPrompt.includes('about'))) {
            return `Sunny is an innovative fintech company that develops proprietary AI models to create smarter, faster, and more secure payment solutions. Founded by ML engineer Samuel Mbugua, Sunny focuses on:\n\n• **AI Innovation** - In-house developed Helios models\n• **Smart Payment Routing** - Optimized success rates\n• **Global Infrastructure** - 150+ currencies supported\n• **Developer-First APIs** - Easy integration\n• **Local AI Models** - Your data stays private\n\nOur mission is to democratize financial services through responsible AI innovation.`;
        }
        
        // General intelligent responses
        const responses = [
            `Based on your question about "${prompt}", I'm analyzing this using Sunny's Helios Reason model. Could you provide more specific details so I can give you the most accurate response?`,
            `That's an interesting inquiry about "${prompt}". As Helios, I'm processing this through Sunny's proprietary AI models. What particular aspect would you like me to focus on?`,
            `I understand you're asking about "${prompt}". Using my advanced reasoning capabilities powered by Sunny's Helios models, I'd be happy to help you explore this topic further. What specific information are you looking for?`
        ];
        
        return responses[Math.floor(Math.random() * responses.length)];
    }

    /**
     * Download and set up actual DeepSeek models
     * This function helps users set up real local models
     */
    async setupLocalModels() {
        console.log('\n🚀 Setting up local DeepSeek models...');
        console.log('\nTo get truly local models, you need to download them from HuggingFace:');
        console.log('\n1. For DeepSeek-Coder (smaller, good for code generation):');
        console.log('   git lfs clone https://huggingface.co/deepseek-ai/deepseek-coder-1.3b-instruct');
        console.log('\n2. For DeepSeek-R1 (larger, better reasoning):');
        console.log('   git lfs clone https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B');
        console.log('\n3. Install git-lfs if you haven\'t already:');
        console.log('   sudo apt install git-lfs');
        console.log('\nNote: These models require significant disk space (1.3B model ~2.6GB, 7B model ~14GB)');
        console.log('\n✨ Currently using intelligent simulation until models are downloaded.');
    }

    /**
     * Check if actual local models are available
     */
    async checkLocalModels() {
        try {
            const configPath = path.join(this.modelsPath, 'helios_config.json');
            await fs.access(configPath);
            
            const config = JSON.parse(await fs.readFile(configPath, 'utf8'));
            return config.downloaded && config.downloaded.length > 0;
        } catch (error) {
            return false;
        }
    }

    /**
     * Generate response using actual downloaded models
     */
    async generateWithLocalModels(prompt, type) {
        try {
            console.log(`🧠 Using local ${type} model for: ${prompt.substring(0, 50)}...`);
            
            // Create a Python script to run the model
            const scriptContent = `
import sys
import json
import torch
from pathlib import Path
from transformers import AutoTokenizer, AutoModelForCausalLM

def generate_response(prompt, model_type):
    models_dir = Path("${this.modelsPath}")
    
    if model_type == "code":
        model_path = models_dir / "deepseek-coder"
    else:
        model_path = models_dir / "deepseek-coder"  # Use coder for now since it's downloaded
    
    try:
        # Load tokenizer and model with optimizations
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        model = AutoModelForCausalLM.from_pretrained(
            model_path, 
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto" if torch.cuda.is_available() else None,
            low_cpu_mem_usage=True
        )
        
        # Tokenize input
        inputs = tokenizer.encode(prompt, return_tensors="pt", truncation=True, max_length=256);
        
        # Generate with optimized settings
        with torch.no_grad():
            outputs = model.generate(
                inputs,
                max_new_tokens=200,  # Reduced for faster generation
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id,
                attention_mask=torch.ones_like(inputs)
            )
        
        # Decode response (only new tokens)
        response = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)
        return response.strip()
        
    except Exception as e:
        return f"Error using local model: {str(e)}"

if __name__ == "__main__":
    prompt = sys.argv[1] if len(sys.argv) > 1 else "Hello"
    model_type = sys.argv[2] if len(sys.argv) > 2 else "reasoning"
    
    result = generate_response(prompt, model_type)
    print(result)
`;
            
            // Write temporary script
            const tempScript = path.join(__dirname, 'temp_generation.py');
            await fs.writeFile(tempScript, scriptContent);
            
            // Run the script
            const result = await this.runPythonScript(tempScript, [prompt, type]);
            
            // Clean up
            await fs.unlink(tempScript).catch(() => {});
            
            return result || this.simulateLocalModel(prompt, type);
            
        } catch (error) {
            console.error('Error with local model:', error);
            return this.simulateLocalModel(prompt, type);
        }
    }

    /**
     * Run Python script with arguments
     */
    async runPythonScript(scriptPath, args = []) {
        return new Promise((resolve, reject) => {
            const python = spawn(this.pythonPath, [scriptPath, ...args]);
            let output = '';
            let error = '';

            python.stdout.on('data', (data) => {
                output += data.toString();
            });

            python.stderr.on('data', (data) => {
                error += data.toString();
            });

            python.on('close', (code) => {
                if (code === 0) {
                    resolve(output.trim());
                } else {
                    console.error('Python script error:', error);
                    reject(new Error(error || 'Python script failed'));
                }
            });

            // Set timeout
            setTimeout(() => {
                python.kill();
                reject(new Error('Python script timeout'));
            }, 30000); // 30 second timeout
        });
    }

    /**
     * Get service status
     */
    async getStatus() {
        const modelsAvailable = await this.checkLocalModels();
        
        return {
            initialized: this.isInitialized,
            venvPath: this.venvPath,
            modelsPath: this.modelsPath,
            pythonPath: this.pythonPath,
            modelsDownloaded: modelsAvailable,
            models: {
                coder: modelsAvailable ? 'local model ready' : 'simulated (download available)',
                reasoner: modelsAvailable ? 'local model ready' : 'simulated (download available)'
            },
            message: modelsAvailable ? 
                'Using actual local DeepSeek models! 🚀' : 
                'Service ready - using intelligent simulation until local models are downloaded'
        };
    }
}

module.exports = new LocalDeepSeekService();

